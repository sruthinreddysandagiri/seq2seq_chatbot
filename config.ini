[modelparam]
# rnn_size:
num_units = 256

# num of layers for cells in encoder and decoder
num_layers = 2

# embeddding_size for every word in dictionary should equals to num_units
embedding_size = 256

# size of dictionary
vocab_size = 24643

# max_gradient bound for every update through backpropagation
max_gradient_norm = 5.0

# beam width for beam search
beam_size = 3

# switch for attention mechanism
use_attention = True

# switch for beam search mechainsm, this is just used in 'inference mode'
use_beam_search = True

# index of start token '<go>' in the dictionary
start_token_idx = 1

# index of end token '<end>' in the dictionary
end_token_idx = 2


[trainparam]
# learning_rate for training
learning_rate = 0.001

# keep probablity of weight during applying dropout
keep_prob = 0.7

# number of loops while training
epochs = 30

# batch size of each inputs during training
batch_size = 256

# save model per 1000 steps
steps_per_checkpoint = 100

# model saved directory
checkpoint_dir = modelsaved/

# model saved file' name
checkpoint_name = seq2seq.ckpt

